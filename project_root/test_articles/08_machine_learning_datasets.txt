Подготовка данных для машинного обучения: очистка, нормализация и feature engineering

Качество данных определяет успех моделей машинного обучения. Афоризм "garbage in, garbage out" особенно актуален — даже лучшие алгоритмы бесполезны на грязных данных. Подготовка датасетов занимает 80% времени data scientist, но критична для точности предсказаний.

Отсутствующие значения (missing values) требуют стратегии обработки. Удаление строк с пропусками теряет данные, допустимо только если пропусков мало. Заполнение средним/медианным значением работает для числовых фич. Для категориальных — создание отдельной категории "unknown". Продвинутый подход — предсказание пропущенных значений моделью.

Выбросы (outliers) искажают статистику и веса моделей. Визуализация boxplot или scatter plot выявляет аномальные точки. Z-score метод удаляет значения, отклоняющиеся на 3+ стандартных отклонения от среднего. IQR (Interquartile Range) метод отсекает за пределами 1.5*IQR от квартилей. Но выбросы могут быть легитимными — анализ бизнес-контекста обязателен.

Нормализация и стандартизация приводят фичи к общему масштабу. Min-max scaling масштабирует в диапазон [0,1], сохраняя распределение. Standardization (z-score) центрирует со средним 0 и стандартным отклонением 1, предполагая нормальное распределение. Модели вроде linear regression, SVM, neural networks чувствительны к масштабу; деревья решений — нет.

Feature engineering создаёт новые признаки из существующих. Из даты извлекаются день недели, месяц, является ли выходным. Логарифмирование сжимает распределения с длинным хвостом. Бинаризация превращает числа в категории (возраст → возрастная группа). Взаимодействия (interaction features) комбинируют признаки (площадь * этаж для цены квартиры).

Кодирование категориальных переменных обязательно, так как большинство алгоритмов работают с числами. One-hot encoding создаёт бинарную колонку для каждой категории. Label encoding присваивает числа, но вводит ложную упорядоченность. Target encoding заменяет категорию средним целевой переменной, но требует осторожности с overfitting.
