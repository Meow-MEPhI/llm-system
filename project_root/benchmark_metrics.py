"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∞ LLM-as-a-Judge —Å–∏—Å—Ç–µ–º—ã.
–ß–∏—Ç–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ –∏–∑ –ø–∞–ø–∫–∏ test_articles/, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏—Ö –∏ —Å–æ–±–∏—Ä–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏.
"""

import os
import sys
import time
import json
from datetime import datetime
from typing import List, Dict
import statistics
from dotenv import load_dotenv
import glob

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'agent_system'))

from agent_system.graph_orchestrator import create_multi_agent_graph

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()


# ==================== –ú–ï–¢–†–ò–ö–ò ====================

class MetricsCollector:
    def __init__(self):
        self.runs: List[Dict] = []

    def add_run(self, run_data: Dict):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–ø—É—Å–∫–∞"""
        self.runs.append(run_data)

    def calculate_statistics(self) -> Dict:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Å–µ–º –∑–∞–ø—É—Å–∫–∞–º"""
        if not self.runs:
            return {}

        latencies = [r['latency'] for r in self.runs if 'latency' in r and r['status'] == 'success']
        tokens = [r['total_tokens'] for r in self.runs if 'total_tokens' in r and r['status'] == 'success']
        errors = [r for r in self.runs if r.get('status') == 'error']

        return {
            'total_runs': len(self.runs),
            'successful_runs': len(self.runs) - len(errors),
            'error_rate': len(errors) / len(self.runs) * 100 if self.runs else 0,
            'latency': {
                'mean': statistics.mean(latencies) if latencies else 0,
                'median': statistics.median(latencies) if latencies else 0,
                'p95': self._percentile(latencies, 0.95) if latencies else 0,
                'p99': self._percentile(latencies, 0.99) if latencies else 0,
                'min': min(latencies) if latencies else 0,
                'max': max(latencies) if latencies else 0,
            },
            'tokens': {
                'mean': statistics.mean(tokens) if tokens else 0,
                'median': statistics.median(tokens) if tokens else 0,
                'total': sum(tokens) if tokens else 0,
                'min': min(tokens) if tokens else 0,
                'max': max(tokens) if tokens else 0,
            },
            'errors': errors
        }

    @staticmethod
    def _percentile(data: List[float], percentile: float) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å"""
        if not data:
            return 0
        sorted_data = sorted(data)
        index = int(len(sorted_data) * percentile)
        return sorted_data[min(index, len(sorted_data) - 1)]

    def print_report(self):
        """–í—ã–≤–æ–¥–∏—Ç –∫—Ä–∞—Å–∏–≤—ã–∏ÃÜ –æ—Ç—á–µÃà—Ç"""
        stats = self.calculate_statistics()

        print("\n" + "=" * 80)
        print("üìä –û–¢–ß–ïÃà–¢ –ü–û –ú–ï–¢–†–ò–ö–ê–ú LLM-AS-A-JUDGE")
        print("=" * 80)

        print(f"\nüìà –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"  ‚Ä¢ –í—Å–µ–≥–æ –∑–∞–ø—É—Å–∫–æ–≤: {stats['total_runs']}")
        print(f"  ‚Ä¢ –£—Å–ø–µ—à–Ω—ã—Ö: {stats['successful_runs']}")
        print(f"  ‚Ä¢ –ü—Ä–æ—Ü–µ–Ω—Ç –æ—à–∏–±–æ–∫: {stats['error_rate']:.2f}%")

        if stats['latency']['mean'] > 0:
            print(f"\n‚è±Ô∏è  Latency (–∑–∞–¥–µ—Ä–∂–∫–∞):")
            print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ: {stats['latency']['mean']:.2f} —Å–µ–∫")
            print(f"  ‚Ä¢ –ú–µ–¥–∏–∞–Ω–∞ (P50): {stats['latency']['median']:.2f} —Å–µ–∫")
            print(f"  ‚Ä¢ P95: {stats['latency']['p95']:.2f} —Å–µ–∫")
            print(f"  ‚Ä¢ P99: {stats['latency']['p99']:.2f} —Å–µ–∫")
            print(f"  ‚Ä¢ Min / Max: {stats['latency']['min']:.2f} / {stats['latency']['max']:.2f} —Å–µ–∫")

        if stats['tokens']['total'] > 0:
            print(f"\nü™ô –¢–æ–∫–µ–Ω—ã:")
            print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –Ω–∞ –∑–∞–ø—É—Å–∫: {stats['tokens']['mean']:.0f}")
            print(f"  ‚Ä¢ –ú–µ–¥–∏–∞–Ω–∞: {stats['tokens']['median']:.0f}")
            print(f"  ‚Ä¢ –í—Å–µ–≥–æ –ø–æ—Ç—Ä–∞—á–µ–Ω–æ: {stats['tokens']['total']:,}")
            print(f"  ‚Ä¢ Min / Max: {stats['tokens']['min']:.0f} / {stats['tokens']['max']:.0f}")

        if stats['errors']:
            print(f"\n‚ùå –û—à–∏–±–∫–∏ ({len(stats['errors'])}):")
            for i, err in enumerate(stats['errors'][:5], 1):  # –ü–µ—Ä–≤—ã–µ 5
                print(f"  {i}. {err.get('title', 'Unknown')}: {err.get('error_message', 'Unknown error')[:80]}...")

        print("\n" + "=" * 80)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
        report_file = f"metrics_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'statistics': stats,
                'runs': self.runs
            }, f, indent=2, ensure_ascii=False)

        print(f"üíæ –ü–æ–ª–Ω—ã–∏ÃÜ –æ—Ç—á–µÃà—Ç —Å–æ—Ö—Ä–∞–Ω–µÃà–Ω –≤: {report_file}\n")


# ==================== –ó–ê–ì–†–£–ó–ö–ê –°–¢–ê–¢–ï–òÃÜ ====================

def load_articles_from_folder(folder_path: str = 'test_articles') -> List[Dict[str, str]]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Å—Ç–∞—Ç—å–∏ –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–∏ÃÜ –ø–∞–ø–∫–∏"""
    if not os.path.exists(folder_path):
        print(f"‚ùå –ü–∞–ø–∫–∞ '{folder_path}' –Ω–µ –Ω–∞–∏ÃÜ–¥–µ–Ω–∞!")
        print(f"üí° –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python generate_test_articles.py")
        return []

    articles = []
    txt_files = sorted(glob.glob(os.path.join(folder_path, '*.txt')))

    if not txt_files:
        print(f"‚ùå –í –ø–∞–ø–∫–µ '{folder_path}' –Ω–µ—Ç .txt —Ñ–∞–∏ÃÜ–ª–æ–≤!")
        return []

    for filepath in txt_files:
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read().strip()

            filename = os.path.basename(filepath)
            title = content.split('\n')[0] if content else filename  # –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –∫–∞–∫ –∑–∞–≥–æ–ª–æ–≤–æ–∫

            articles.append({
                'filename': filename,
                'title': title[:100],  # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
                'text': content
            })
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {filepath}: {e}")

    return articles


# ==================== –ó–ê–ü–£–°–ö –ë–ï–ù–ß–ú–ê–†–ö–ê ====================

def run_benchmark(num_articles: int = None):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –±–µ–Ω—á–º–∞—Ä–∫ –Ω–∞ —Å—Ç–∞—Ç—å—è—Ö –∏–∑ –ø–∞–ø–∫–∏"""

    print("\n" + "üöÄ –ó–ê–ü–£–°–ö –ë–ï–ù–ß–ú–ê–†–ö–ê LLM-AS-A-JUDGE" + "\n")
    print(f"–í—Ä–µ–º—è —Å—Ç–∞—Ä—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ AUTH KEY
    auth_key = os.getenv('GIGACHAT_AUTH_KEY')
    if not auth_key:
        print("‚ùå GIGACHAT_AUTH_KEY –Ω–µ –Ω–∞–∏ÃÜ–¥–µ–Ω –≤ .env!")
        return

    # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–µ–∏ÃÜ
    print("\nüìÅ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–µ–∏ÃÜ –∏–∑ –ø–∞–ø–∫–∏ 'test_articles/'...")
    articles = load_articles_from_folder()

    if not articles:
        return

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ
    if num_articles:
        articles = articles[:num_articles]

    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–∏ÃÜ\n")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    collector = MetricsCollector()

    try:
        # –°–æ–∑–¥–∞–µÃà–º –≥—Ä–∞—Ñ –æ–¥–∏–Ω —Ä–∞–∑
        print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞ –∞–≥–µ–Ω—Ç–æ–≤...")
        graph = create_multi_agent_graph(auth_key=auth_key)
        print("‚úÖ –ì—Ä–∞—Ñ —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∞: {e}")
        return

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç–µ–∏ÃÜ
    for idx, article in enumerate(articles, 1):
        print(f"\n{'‚îÄ' * 80}")
        print(f"üìÑ –°—Ç–∞—Ç—å—è {idx}/{len(articles)}: {article['filename']}")
        print(f"üìù {article['title'][:70]}...")
        print(f"{'‚îÄ' * 80}")

        start_time = time.time()

        initial_state = {
            "article_text": article['text'],
            "rubric_result_rubricator": "",
            "rubric_result_keyword": "",
            "rubric_result_normal": "",
            "rubric_result_summariser": "",
            "critique": "",
            "critique_key": "",
            "critique_sum": "",
            "critique_nor": "",
            "revision_count": 0,
            "revision_count_key": 0,
            "revision_count_sum": 0,
            "revision_count_nor": 0,
            "status": ["started"]
        }

        try:
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≥—Ä–∞—Ñ
            final_state = graph.invoke(initial_state)

            latency = time.time() - start_time

            # –ü—Ä–∏–º–µ—Ä–Ω—ã–∏ÃÜ –ø–æ–¥—Å—á–µÃà—Ç —Ç–æ–∫–µ–Ω–æ–≤ (–¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ / 4)
            total_tokens = (
                                   len(article['text']) +
                                   len(final_state.get('rubric_result_rubricator', '')) +
                                   len(final_state.get('rubric_result_keyword', '')) +
                                   len(final_state.get('rubric_result_normal', '')) +
                                   len(final_state.get('rubric_result_summariser', ''))
                           ) // 4

            run_data = {
                'article_id': idx,
                'filename': article['filename'],
                'title': article['title'],
                'status': 'success',
                'latency': latency,
                'total_tokens': total_tokens,
                'revision_count': final_state.get('revision_count', 0),
                'results': {
                    'rubric': final_state.get('rubric_result_rubricator', '')[:100],
                    'keywords': final_state.get('rubric_result_keyword', '')[:100],
                    'summary': final_state.get('rubric_result_summariser', '')[:100],
                }
            }

            collector.add_run(run_data)

            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –∑–∞ {latency:.2f}—Å (‚âà{total_tokens:,} —Ç–æ–∫–µ–Ω–æ–≤)")
            print(f"   üè∑Ô∏è  –†—É–±—Ä–∏–∫–∞: {run_data['results']['rubric'][:60]}...")

        except Exception as e:
            latency = time.time() - start_time
            error_msg = str(e)

            run_data = {
                'article_id': idx,
                'filename': article['filename'],
                'title': article['title'],
                'status': 'error',
                'latency': latency,
                'error_message': error_msg
            }

            collector.add_run(run_data)

            print(f"‚ùå –û—à–∏–±–∫–∞: {error_msg[:150]}")

        # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        if idx < len(articles):
            time.sleep(2)

    # –ò—Ç–æ–≥–æ–≤—ã–∏ÃÜ –æ—Ç—á–µÃà—Ç
    collector.print_report()


# ==================== MAIN ====================

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='–ë–µ–Ω—á–º–∞—Ä–∫ LLM-as-a-Judge —Å–∏—Å—Ç–µ–º—ã')
    parser.add_argument('-n', '--num', type=int, default=None,
                        help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–∏ÃÜ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –≤—Å–µ)')

    args = parser.parse_args()

    run_benchmark(num_articles=args.num)
